{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Roche Capstone - Advanced Model Selection Engine (Systematic Analysis)\n",
                "\n",
                "## 1. Introduction: Moving Beyond Simple Fitting\n",
                "This notebook is not just about training a model; it is a **Comparative Analysis** designed to identify the optimal architecture for predicting lab delays in the Roche simulation environment.\n",
                "\n",
                "We evaluate three classes of models:\n",
                "1.  **Baseline (Linear Regression)**: To establish a performance floor and check for linear relationships.\n",
                "2.  **Challenger 1 (Random Forest)**: A robust ensemble method excellent for handling non-linear operational data and outliers.\n",
                "3.  **Challenger 2 (XGBoost)**: The current 'State of the Art' for tabular data, capable of modeling subtle interactions and handling missing values natively.\n",
                "\n",
                "Our primary metric is **MAE (Mean Absolute Error)**, as we want to minimize the number of minutes our prediction is off by."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "import xgboost as xgb\n",
                "\n",
                "from sklearn.metrics import mean_absolute_error, r2_score\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Engineering: The 'Physics' Layer\n",
                "Raw data is rarely enough. To capture the operational complexities, we engineer interaction terms that reflect reality:\n",
                "*   **Stress_Index**: A multiplicative interaction of `Scientist_Workload` and `Lab_Occupancy`. High occupancy alone is manageable, but when combined with high workload, it creates a disaster (bottleneck).\n",
                "*   **Hour_of_Day**: Extracted from timestamps to capture the 'Lunchtime Rush' and daily cycles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "df_wk = pd.read_csv('workflow_logs.csv')\n",
                "df_rg = pd.read_csv('reagent_logs.csv')\n",
                "df_tl = pd.read_csv('telemetry_logs.csv')\n",
                "\n",
                "# Merge Data Features\n",
                "df = df_wk.merge(df_rg[['experiment_id', 'reagent_batch_id']], on='experiment_id', how='left')\n",
                "df = df.merge(df_tl[['experiment_id', 'ambient_temp']], on='experiment_id', how='left')\n",
                "\n",
                "# --- Feature Engineering ---\n",
                "# 1. Drift/Aging\n",
                "df['booking_time'] = pd.to_datetime(df['booking_time'])\n",
                "start_time = df['booking_time'].min()\n",
                "df['days_since_start'] = (df['booking_time'] - start_time).dt.days\n",
                "\n",
                "# 2. Hour of Day (Daily Cycle)\n",
                "df['hour_of_day'] = df['booking_time'].dt.hour\n",
                "\n",
                "# 3. Stress Index (Interaction Term)\n",
                "df['stress_index'] = df['scientist_workload'] * df['lab_occupancy_level']\n",
                "\n",
                "# 4. Target\n",
                "df = df.dropna(subset=['delay'])\n",
                "\n",
                "print(\"Feature Engineering Complete.\")\n",
                "print(f\"New Shape: {df.shape}\")\n",
                "display(df[['stress_index', 'hour_of_day', 'days_since_start']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Model Zoo (Benchmarks)\n",
                "We construct our training pipeline. We will use a standard `ColumnTransformer` to handle preprocessing:\n",
                "-   **Numerical**: Impute Missing + Scale.\n",
                "-   **Categorical**: One-Hot Encode (handling unknown categories). \n",
                "\n",
                "All models will use the exact same preprocessing to ensure a fair fight."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Features Definition\n",
                "NUM_FEATURES = ['scientist_workload', 'lab_occupancy_level', 'expected_duration', 'ambient_temp', \n",
                "                'days_since_start', 'hour_of_day', 'stress_index']\n",
                "CAT_FEATURES = ['experiment_type', 'instrument_type', 'scientist_experience_level', 'reagent_batch_id']\n",
                "TARGET = 'delay'\n",
                "\n",
                "X = df[NUM_FEATURES + CAT_FEATURES]\n",
                "y = df[TARGET]\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Preprocessing Pipeline\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]), NUM_FEATURES),\n",
                "        ('cat', OneHotEncoder(handle_unknown='ignore'), CAT_FEATURES)\n",
                "    ])\n",
                "\n",
                "# Initialize Models\n",
                "models = {\n",
                "    \"Baseline (Linear Regression)\": LinearRegression(),\n",
                "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
                "    \"XGBoost\": xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
                "}\n",
                "\n",
                "print(\"Models Initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation & Selection\n",
                "We evaluate each model using **MAE (Mean Absolute Error)**. \n",
                "-   **Why MAE?** It is interpretable. \"On average, our prediction is off by X minutes\". RMSE is too sensitive to the massive outliers (Gamma tail) in our data.\n",
                "-   **R2 Score**: Shows how much variance is explained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "trained_pipelines = {}\n",
                "\n",
                "print(\"Training & Evaluating Models...\\n\")\n",
                "\n",
                "for name, model in models.items():\n",
                "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
                "                               ('model', model)])\n",
                "    \n",
                "    pipeline.fit(X_train, y_train)\n",
                "    y_pred = pipeline.predict(X_test)\n",
                "    \n",
                "    mae = mean_absolute_error(y_test, y_pred)\n",
                "    r2 = r2_score(y_test, y_pred)\n",
                "    \n",
                "    results.append({\"Model\": name, \"MAE (Min)\": mae, \"R2 Score\": r2})\n",
                "    trained_pipelines[name] = pipeline\n",
                "    print(f\"{name}: MAE = {mae:.2f} min, R2 = {r2:.4f}\")\n",
                "\n",
                "# Comparison Table\n",
                "results_df = pd.DataFrame(results).sort_values(\"MAE (Min)\")\n",
                "display(results_df)\n",
                "\n",
                "best_model_name = results_df.iloc[0]['Model']\n",
                "best_pipeline = trained_pipelines[best_model_name]\n",
                "print(f\"\\nChampion Model Selected: {best_model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interpretability (The 'Why')\n",
                "In Pharma/Lab Ops, a black-box model is dangerous. We must understand **what drives the delay**.\n",
                "We analyze the Feature Importance of the Champion Model to confirm physics-based expectations:\n",
                "1.  **Is Stress_Index a driver?**\n",
                "2.  **Are the Bad Batches (e.g., BATCH_392) detected?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract Feature Importances\n",
                "if best_model_name in ['Random Forest', 'XGBoost']:\n",
                "    model_step = best_pipeline.named_steps['model']\n",
                "    preproc_step = best_pipeline.named_steps['preprocessor']\n",
                "    \n",
                "    # Get feature names from transformers\n",
                "    cat_names = preproc_step.named_transformers_['cat'].get_feature_names_out(CAT_FEATURES)\n",
                "    feature_names = NUM_FEATURES + list(cat_names)\n",
                "    \n",
                "    importances = model_step.feature_importances_\n",
                "    \n",
                "    # Create DF\n",
                "    imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
                "    imp_df = imp_df.sort_values(by='Importance', ascending=False).head(15)\n",
                "    \n",
                "    # Cleanup BATCH names for readability\n",
                "    imp_df['Feature'] = imp_df['Feature'].apply(lambda x: x.replace('reagent_batch_id_', 'Batch: '))\n",
                "    \n",
                "    # Plot\n",
                "    plt.figure(figsize=(10, 8))\n",
                "    sns.barplot(x='Importance', y='Feature', data=imp_df, palette='viridis')\n",
                "    plt.title(f'Feature Importance: Top Drivers of Delay ({best_model_name})')\n",
                "    plt.xlabel('Importance Score')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Linear models require coefficients analysis, skipping tree-based importance plot.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Productionizing\n",
                "We save the Champion Model to a pickle file (`lab_delay_model_v2.pkl`) so it can be deployed in the Streamlit App."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib\n",
                "\n",
                "# Save the best pipeline\n",
                "filename = 'lab_delay_model_v2.pkl'\n",
                "joblib.dump(best_pipeline, filename)\n",
                "print(f\"Champion Model ({best_model_name}) saved to {filename}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}